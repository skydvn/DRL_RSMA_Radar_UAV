{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DRL-RSMA-Radar.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mpllzjbjn6lb",
        "DNgW_zzrn_3y",
        "ek73oLa8bjjE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "mpllzjbjn6lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/reinforcement-learning/DRL-RSMA-Radar\n",
        "!ls"
      ],
      "metadata": {
        "id": "j4wirn3GgNvw",
        "outputId": "5959587a-d987-4c95-cd34-f4d26c367100",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/reinforcement-learning/DRL-RSMA-Radar'\n",
            "/content\n",
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !apt install python-opengl\n",
        "    !apt install ffmpeg\n",
        "    !apt install xvfb\n",
        "    !pip install pyvirtualdisplay\n",
        "    from pyvirtualdisplay import Display\n",
        "    \n",
        "    # Start virtual display\n",
        "    dis = Display(visible=0, size=(600, 400))\n",
        "    dis.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH0dOjduMJZE",
        "outputId": "27990862-e4c7-4ffa-b0d0-b537e921d8d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 1s (669 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 155629 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library definition"
      ],
      "metadata": {
        "id": "DNgW_zzrn_3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fundamental\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "import random\n",
        "\n",
        "import scipy\n",
        "from scipy import special\n",
        "from scipy.special import lambertw\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pytorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "dtype = np.float32"
      ],
      "metadata": {
        "id": "5Gf0XAdGoDms"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU \n",
        "if torch.backends.cudnn.enabled:\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    print(\"GPU enabled\")\n",
        "\n",
        "seed = 777\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WizJlJScMDRK",
        "outputId": "8f5bb2e0-c58e-4b36-df44-048b60fd2c80"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU enabled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment "
      ],
      "metadata": {
        "id": "bRzGOQ-xn-pP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definition of Channel\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ek73oLa8bjjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BS - CU**\n",
        "* Channel: $h^C_q$, Noise: $n^C_q$ ~ $\\sigma^2_q$, Power: $p_o$ (broadcast power), $p_q$ (normal transmit power)\n",
        "\n",
        "**CU**\n",
        "* Common data rate: $a_{q,0}$\n",
        "\n",
        "**UAV**\n",
        "* Channel: $h^C_q$, Noise: $n^C_q$ ~ $\\sigma^2_q$\n",
        "\n",
        "**UAV**\n",
        "* Channel: $h^C_q$ \n",
        "<!-- *  Noise:   $n^C_q$ ~ $\\sigma^2_q$ -->\n",
        "\n",
        "**Radar**\n",
        "* Channel: $h^{CR}$, Noise: $n^R$ ~ $\\sigma^2$\n",
        "\n",
        "<!-- **Radar - UAV**\n",
        "* Channel: $h^R$, Noise: $n^R$ ~ $\\sigma^2$ -->\n"
      ],
      "metadata": {
        "id": "ABQldJO2er8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "fGVutbBQesxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNUR settings\n",
        "Here is a CNUR settings\n",
        "\n",
        "| Notations            | Value        ||| Notations            | Value        | \n",
        "|---                   |---           ||| ---                  | ---          | \n",
        "|Number of CU (Q)      | 10 - 20      ||| Antenna Gain T BS    |  17dBi~50    | \n",
        "|Wave length (lambda)  | 0.1m         ||| Antenna Gain R BS    |   0dBi~1     | \n",
        "|BS to CU              | 200-300m     ||| Antanna Gain Radar   |  30dBi~1000  | \n",
        "|Radar to CU           | 1000-2000m   ||| G'^R_t               | -27dBi~0.002 | \n",
        "|Radar to UAV          | 5000-10000m  ||| G'^R_r               | -27dBi~0.002 | \n",
        "|P_BS                  | 30 dBm / 1W  ||| sigma_RCS            | 1m^2         | \n",
        "|P_radar               | 1000W        ||| sigma^2_q, sigma^2   | -150dBm/Hz   | |B                     | 10^6         ||| C^TH                 | 10^5 - 4*10^5| "
      ],
      "metadata": {
        "id": "A1Hu2XGdiQF1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GwXtcdjfejiX"
      },
      "outputs": [],
      "source": [
        "class Env_CNUR():\n",
        "    ### Initialization ###\n",
        "    def __init__(self, MAX_EP_STEPS, N_user_max,   \n",
        "                 BS_R_min, BS_R_max, radar_R,           # Radius of BS and Radar distance to BS\n",
        "                 h_max, h_min,                          # Height of the UAV\n",
        "                 uav_R_max, uav_R_min,                  # Radius of the UAV\n",
        "                #  Radar_R_max,                           # Radius of the Radar\n",
        "                 C_TH,                                  # Datarate threshold\n",
        "                 P_max, Bandwidth, noise):              # Power, Data Rate, Bandwidth\n",
        "        # Visualization\n",
        "        self.visualization = \"2D\"          # 3D or else\n",
        "        # Data logging\n",
        "        self.verbose_distance = False\n",
        "        self.verbose_channelGain = True\n",
        "\n",
        "        # Network settings\n",
        "        self.BS_R_max = BS_R_max           # Base Station maximum radius\n",
        "        self.BS_R_min = BS_R_min           # Base Station minimum radius\n",
        "        self.N_User = N_user_max           # Number of User / m\n",
        "        self.P_max  = P_max                # Base Station 's max power / m\n",
        "        self.noise  = noise                # Noise density: -174 dBm/Hz\n",
        "        self.B      = Bandwidth            # Bandwidth\n",
        "        self.C_TH   = C_TH                 # Lower threshold for data rate - CU\n",
        "\n",
        "        self.n_0 = noise**2 * self.B       # Noise power       \n",
        "\n",
        "        # Antenna Gain\n",
        "        self.G_R_t = 1                                  # Transmitting Radar Gain\n",
        "        self.G_R_r = 1                                  # Transmitting Radar Gain\n",
        "        self.G_C_t = 1                                  # Transmitting BS Gain\n",
        "        self.G_CU_list = np.ones((self.N_User,1))       # Receiving antenna gain of all CUs\n",
        "        self.sigma_RCS = 0                              # Radar coss section of target - Radar\n",
        "\n",
        "        # Base Station initialization\n",
        "        self.P_max = P_max                 # Max power        \n",
        "        self.BS_x = 0                      # BS location initialization \n",
        "        self.BS_y = 0                      # BS location initialization \n",
        "        self.BS_location = np.expand_dims(self.location_BS_Generator(), axis=0)\n",
        "\n",
        "        # Radar initialization\n",
        "        self.Radar_R = radar_R             # Radius\n",
        "        self.Radar_location = np.expand_dims(self.location_Radar_Generator(), axis=0)\n",
        "\n",
        "        # UAV initialization\n",
        "        self.uav_h_max = h_max             # Height max\n",
        "        self.uav_h_min = h_min             # Height min\n",
        "        self.uav_R_max = uav_R_max         # R UAV max\n",
        "        self.uav_R_min = uav_R_min         # R UAV min\n",
        "        self.UAV_location   = np.expand_dims(self.location_UAV_Generator(), axis=0)\n",
        "\n",
        "        # Communication User setting\n",
        "        # User location initialization\n",
        "        self.CU_location = self.location_CU_Generator()\n",
        "\n",
        "        \n",
        "        # Distance calculation\n",
        "\n",
        "        self.distance_Radar_UAV    = self._calculateDistance(self.UAV_location, self.Radar_location)\n",
        "        self.distance_BS_Radar     = self._calculateDistance(self.UAV_location, self.BS_location)\n",
        "        self.distanceList_BS_CU    = self._calculateDistance(self.BS_location , self.CU_location)\n",
        "        self.distanceList_Radar_CU = self._calculateDistance(self.CU_location , self.Radar_location)\n",
        "        if self.verbose_distance == True:\n",
        "            print(\"Distance\")\n",
        "            print(f\"radar-uav: {self.distance_Radar_UAV}\")\n",
        "            print(f\"radar-bs: {self.distance_BS_Radar}\")\n",
        "            print(f\"bs-cu: {self.distanceList_BS_CU}\")\n",
        "            print(f\"radar-cu: {self.distanceList_Radar_CU}\")\n",
        "         \n",
        "\n",
        "        # Pathloss calculation\n",
        "\n",
        "        # Channel Gain\n",
        "        self.H_R  = self.channelGain_Radar_UAV()           # Channel round-trip between Radar and UAV\n",
        "        self.H_CR = self.channelGain_BS_Radar()             # Channel from BS to Radar\n",
        "        self.H_C  = self.channelGain_BS_CU()               # Channel from BS to CU    (Interference)\n",
        "        self.H_RC = self.channelGain_Radar_CU()         # Channel from Radar to CU (Interference)\n",
        "        if self.verbose_channelGain == True:\n",
        "            print(\"Channel Gain\")\n",
        "            print(f\"radar-uav: {self.H_R}\")\n",
        "            print(f\"radar-bs: {self.H_CR}\")\n",
        "            print(f\"bs-cu: {self.H_C}\")\n",
        "            print(f\"radar-cu: {self.H_RC}\")            \n",
        "\n",
        "\n",
        "        # Environment settings\n",
        "        \"state_size can be represented by number of user initialized each episode\"\n",
        "        self.state_size = N_user_max\n",
        "        self.action_size = 2*N_user_max + 2\n",
        "        self.s_dim = s_dim\n",
        "\n",
        "\n",
        "    ### Functions ###\n",
        "    # Channel gain generation\n",
        "    def channelGain_BS_CU(self):\n",
        "        numerator   = self.G_C_t * self.G_CU_list * (self.lambda_c**2)\n",
        "        denominator = ((4*np.pi)**3) * (self.distanceList_BS_CU**4)\n",
        "        channelGain = numerator/denominator \n",
        "        return channelGain\n",
        "\n",
        "    def channelGain_Radar_CU(self):\n",
        "        numerator   = self.G_R_t * self.G_CU_list * (self.lambda_c**2)\n",
        "        denominator = ((4*np.pi)**3) * (self.distanceList_Radar_CU**4)\n",
        "        channelGain = numerator/denominator \n",
        "        return channelGain\n",
        "\n",
        "    def channelGain_BS_Radar(self):\n",
        "        numerator   = self.G_C_t * self.G_R_r * (self.lambda_c**2)\n",
        "        denominator = ((4*np.pi)**3) * (self.distance_BS_Radar**4)\n",
        "        channelGain = numerator/denominator \n",
        "        return channelGain\n",
        "\n",
        "    def channelGain_Radar_UAV(self):            # h^R\n",
        "        numerator   = self.G_R_t * self.G_R_r * self.sigma_RCS * (self.lambda_c**2)\n",
        "        denominator = ((4*np.pi)**3) * (self.distance_Radar_UAV**4)\n",
        "        channelGain = numerator/denominator \n",
        "        return channelGain\n",
        "\n",
        "    # Radar initialization\n",
        "    def location_BS_Generator(self):\n",
        "        # r = self.BS_R_max * np.sqrt(np.random.rand())\n",
        "        # theta = np.random.uniform(-np.pi, np.pi)\n",
        "        # Radar_x = self.BS_x + r*np.cos(theta)\n",
        "        # Radar_y = self.BS_y + r*np.sin(theta)\n",
        "        BS_location = [self.BS_x, self.BS_y, 0]\n",
        "        return np.array(BS_location)\n",
        "\n",
        "    # Radar initialization\n",
        "    def location_Radar_Generator(self):\n",
        "        r = self.Radar_R * np.sqrt(np.random.rand())\n",
        "        theta = np.random.uniform(-np.pi, np.pi)\n",
        "        Radar_x = self.BS_x + r*np.cos(theta)\n",
        "        Radar_y = self.BS_y + r*np.sin(theta)\n",
        "        Radar_location = [Radar_x, Radar_y, 0]\n",
        "        return np.array(Radar_location)\n",
        "\n",
        "    # UAV initialization\n",
        "    def location_UAV_Generator(self):\n",
        "        # h_min < h < h_max\n",
        "        # -180 < theta < 180 (Oxy hyper-space)\n",
        "        # 0 < R < R_Max\n",
        "        r = self.uav_R_max * np.sqrt(np.random.rand())\n",
        "        theta = np.random.uniform(-np.pi, np.pi)\n",
        "        UAV_x = self.BS_x + r*np.cos(theta)\n",
        "        UAV_y = self.BS_y + r*np.sin(theta)\n",
        "        UAV_h = np.random.uniform(self.uav_h_min, self.uav_h_max)\n",
        "        UAV_location = [UAV_x, UAV_y, UAV_h]\n",
        "        return np.array(UAV_location)\n",
        "\n",
        "    # User distance initialization\n",
        "    def location_CU_Generator(self):\n",
        "        # Generate random CU locations\n",
        "        # N_User: number of users\n",
        "        # BS_R_min, BS_R_max: circle radius\n",
        "        # [BS_x, BS_y]: center of the circle\n",
        "        userList = []\n",
        "        for i in range(self.N_User):\n",
        "            r = self.BS_R_max * np.sqrt(np.random.rand())\n",
        "            # theta =  i/self.N_User*2*np.pi + np.random.uniform(-np.pi/10, np.pi/10)\n",
        "            theta = np.random.uniform(-np.pi, np.pi)\n",
        "            xUser_temp = self.BS_x + r*np.cos(theta)\n",
        "            yUser_temp = self.BS_y + r*np.sin(theta)\n",
        "            userList.append([xUser_temp, yUser_temp, 0])\n",
        "            \n",
        "        comm_user = np.array(userList)\n",
        "        if self.visualization == \"2D\":\n",
        "            #Plot scatter figure\n",
        "            fig, ax = plt.subplots()\n",
        "            circle1 = plt.Circle((self.BS_x, self.BS_y), 1, color='r', fill=True)\n",
        "            circle2 = plt.Circle((self.BS_x, self.BS_y), self.BS_R_min, color='g', fill=False)\n",
        "            circle3 = plt.Circle((self.BS_x, self.BS_y), self.BS_R_max, color='b', fill=False)\n",
        "            ax.add_patch(circle1)\n",
        "            ax.add_patch(circle2)\n",
        "            ax.add_patch(circle3)\n",
        "            # print(comm_user.transpose()[0:2].transpose())\n",
        "            gu = comm_user.transpose()[0:2].transpose()\n",
        "            plt.scatter(gu[:,0], gu[:,1])\n",
        "            plt.show()\n",
        "        # elif self.visualization == \"3D\":\n",
        "        #     fig = plt.figure()\n",
        "        #     ax = fig.add_subplot(projection='3d')\n",
        "        #     circle1 = plt.Circle((self.BS_x, self.BS_y, 0), 1, color='r', fill=True)\n",
        "        #     circle2 = plt.Circle((self.BS_x, self.BS_y, 0), self.BS_R_min, color='g', fill=False)\n",
        "        #     circle3 = plt.Circle((self.BS_x, self.BS_y, 0), self.BS_R_max, color='b', fill=False)\n",
        "        #     n = 100\n",
        "\n",
        "        #     # For each set of style and range settings, plot n random points in the box\n",
        "        #     # defined by x in [23, 32], y in [0, 100], z in [zlow, zhigh].\n",
        "        #     for m, zlow, zhigh in [('o', -50, -25), ('^', -30, -5)]:\n",
        "        #         xs = randrange(n, 23, 32)\n",
        "        #         ys = randrange(n, 0, 100)\n",
        "        #         zs = randrange(n, zlow, zhigh)\n",
        "        #         ax.scatter(xs, ys, zs, marker=m)\n",
        "\n",
        "        #     ax.set_xlabel('X')\n",
        "        #     ax.set_ylabel('Y')\n",
        "        #     ax.set_zlabel('Height')\n",
        "\n",
        "        #     plt.show()            \n",
        "        return comm_user\n",
        "\n",
        "    # SINR Calculator\n",
        "    def SINR_Radar(self, P_radar, H_R, H_CR):\n",
        "        SINR = 1\n",
        "        return SINR\n",
        "\n",
        "    def _calculateDistance(self, A, B):\n",
        "        print(f\"A: {A}\")\n",
        "        print(f\"B: {B}\")\n",
        "        return np.sqrt(np.sum((A - B)**2,axis=1))\n",
        "\n",
        "    ###########################\n",
        "    # DRL Environment process #\n",
        "    def step(self, action, state, j):\n",
        "        reward = 0\n",
        "        state_next = 0\n",
        "        done = 0\n",
        "        return reward, state_next, done\n",
        "\n",
        "    def step_greedy(self,   state, j):\n",
        "        reward = 0\n",
        "        state_next = 0\n",
        "        done = 0\n",
        "        return reward, state_next, done\n",
        "\n",
        "    def step_random(self,   state, j):\n",
        "        reward = 0\n",
        "        state_next = 0\n",
        "        done = 0\n",
        "        return reward, state_next, done\n",
        "\n",
        "    def reset(self):\n",
        "        reward = 0\n",
        "        state_next = 0\n",
        "        done = 0\n",
        "        return state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = Env_CNUR(MAX_EP_STEPS = 10000, N_user_max = 50,   \n",
        "               BS_R_min = 10, BS_R_max = 150, radar_R = 1000,     # Radius of BS and Radar distance to BS\n",
        "               h_max = 500, h_min = 10,                           # Height of the UAV\n",
        "               uav_R_max = 1000, uav_R_min = 200,                 # Radius of the UAV\n",
        "               C_TH = 50,                                         # Datarate threshold\n",
        "               P_max = 100, Bandwidth = 100, noise = -174)        # Power, Data Rate, Bandwidth"
      ],
      "metadata": {
        "id": "KNWzAnU1CWZp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "f9bff01a-1b79-48c0-88d1-8eafc1805875"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8f0459fb5a93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                \u001b[0muav_R_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muav_R_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0;31m# Radius of the UAV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                \u001b[0mC_TH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m                                         \u001b[0;31m# Datarate threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                P_max = 100, Bandwidth = 100, noise = -174)        # Power, Data Rate, Bandwidth\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-3777d58244d7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, MAX_EP_STEPS, N_user_max, BS_R_min, BS_R_max, radar_R, h_max, h_min, uav_R_max, uav_R_min, C_TH, P_max, Bandwidth, noise)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG_R_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m                                  \u001b[0;31m# Transmitting Radar Gain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG_C_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m                                  \u001b[0;31m# Transmitting BS Gain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG_CU_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_User\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# Receiving antenna gain of all CUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_RCS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m                              \u001b[0;31m# Radar coss section of target - Radar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DDPG Agent"
      ],
      "metadata": {
        "id": "6rYoxokfLYEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replay buffer\n",
        "Typically, people implement replay buffers with one of the following three data structures:\n",
        "\n",
        "- collections.deque\n",
        "- list\n",
        "- numpy.ndarray\n",
        "\n",
        "**deque** is very easy to handle once you initialize its maximum length (e.g. deque(maxlen=buffer_size)). However, the indexing operation of deque gets terribly slow as it grows up because it is [internally doubly linked list](https://wiki.python.org/moin/TimeComplexity#collections.deque). On the other hands, **list** is an array, so it is relatively faster than deque when you sample batches at every step. Its amortized cost of Get item is [O(1)](https://wiki.python.org/moin/TimeComplexity#list).\n",
        "\n",
        "Last but not least, let's see **numpy.ndarray**. numpy.ndarray is even faster than list due to the fact that it is [a homogeneous array of fixed-size items](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray), so you can get the benefits of [locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference), . Whereas list is an array of pointers to objects, even when all of them are of the same type.\n",
        "\n",
        "Here, we are going to implement a replay buffer using numpy.ndarray.\n",
        "\n",
        "Reference: \n",
        "- [OpenAI spinning-up](https://github.com/openai/spinningup/blob/master/spinup/algos/sac/sac.py#L10)\n",
        "- [rainbow-is-all-you-need](https://render.githubusercontent.com/view/ipynb?commit=032d11277cf2436853478a69ca5a4aba03202598&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f437572742d5061726b2f7261696e626f772d69732d616c6c2d796f752d6e6565642f303332643131323737636632343336383533343738613639636135613461626130333230323539382f30312e64716e2e6970796e62&nwo=Curt-Park%2Frainbow-is-all-you-need&path=01.dqn.ipynb&repository_id=191133946&repository_type=Repository#Replay-buffer)"
      ],
      "metadata": {
        "id": "Xr5CEJAANWY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_dim: int, size: int, batch_size: int = 32):\n",
        "        \"\"\"Initializate.\"\"\"\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray, \n",
        "        rew: float, \n",
        "        next_obs: np.ndarray, \n",
        "        done: bool,\n",
        "    ):\n",
        "        \"\"\"Store the transition in buffer.\"\"\"\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "        return dict(obs=self.obs_buf[idxs],\n",
        "                    next_obs=self.next_obs_buf[idxs],\n",
        "                    acts=self.acts_buf[idxs],\n",
        "                    rews=self.rews_buf[idxs],\n",
        "                    done=self.done_buf[idxs])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ],
      "metadata": {
        "id": "r2U6kzQgLa4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OU Noise\n",
        "**Ornstein-Uhlenbeck** process generates temporally correlated exploration, and it effectively copes with physical control problems of inertia.\n",
        "\n",
        "$$\n",
        "dx_t = \\theta(\\mu - x_t) dt + \\sigma dW_t\n",
        "$$"
      ],
      "metadata": {
        "id": "5xUhLjdsNRlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OUNoise:\n",
        "    \"\"\"Ornstein-Uhlenbeck process.\n",
        "    Taken from Udacity deep-reinforcement-learning github repository:\n",
        "    https://github.com/udacity/deep-reinforcement-learning/blob/master/\n",
        "    ddpg-pendulum/ddpg_agent.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        size: int, \n",
        "        mu: float = 0.0, \n",
        "        theta: float = 0.15, \n",
        "        sigma: float = 0.2,\n",
        "    ):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.state = np.float64(0.0)\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self) -> np.ndarray:\n",
        "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.array(\n",
        "            [random.random() for _ in range(len(x))]\n",
        "        )\n",
        "        self.state = x + dx\n",
        "        return self.state"
      ],
      "metadata": {
        "id": "jVpi-esHMevz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network"
      ],
      "metadata": {
        "id": "TsvTlYXDNfJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_dim: int, \n",
        "        out_dim: int,\n",
        "        init_w: float = 3e-3,\n",
        "    ):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "        \n",
        "        self.hidden1 = nn.Linear(in_dim, 128)\n",
        "        self.hidden2 = nn.Linear(128, 128)\n",
        "        self.out = nn.Linear(128, out_dim)\n",
        "        \n",
        "        self.out.weight.data.uniform_(-init_w, init_w)\n",
        "        self.out.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        x = F.relu(self.hidden1(state))\n",
        "        x = F.relu(self.hidden2(x))\n",
        "        action = self.out(x).tanh()\n",
        "        \n",
        "        return action\n",
        "    \n",
        "    \n",
        "class Critic(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_dim: int, \n",
        "        init_w: float = 3e-3,\n",
        "    ):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "        \n",
        "        self.hidden1 = nn.Linear(in_dim, 128)\n",
        "        self.hidden2 = nn.Linear(128, 128)\n",
        "        self.out = nn.Linear(128, 1)\n",
        "        \n",
        "        self.out.weight.data.uniform_(-init_w, init_w)\n",
        "        self.out.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(\n",
        "        self, state: torch.Tensor, action: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        x = torch.cat((state, action), dim=-1)\n",
        "        x = F.relu(self.hidden1(x))\n",
        "        x = F.relu(self.hidden2(x))\n",
        "        value = self.out(x)\n",
        "        \n",
        "        return value"
      ],
      "metadata": {
        "id": "hkQ83MuwMiIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DDPG Agent\n",
        "Here is a summary of DDPGAgent class.\n",
        "\n",
        "| Method           | Note                                                 |\n",
        "|---               |---                                                  |\n",
        "|select_action     | select an action from the input state.               |\n",
        "|step              | take an action and return the response of the env.   |\n",
        "|update_model      | update the model by gradient descent.                |\n",
        "|train             | train the agent during num_frames.                   |\n",
        "|test              | test the agent (1 episode).                          |\n",
        "|\\_target_soft_update| soft update from the local model to the target model.|\n",
        "|\\_plot              | plot the training progresses.     "
      ],
      "metadata": {
        "id": "7aAkNYiZNLjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPGAgent:\n",
        "    \"\"\"DDPGAgent interacting with environment.\n",
        "    \n",
        "    Attribute:\n",
        "        env (gym.Env): openAI Gym environment\n",
        "        actor (nn.Module): target actor model to select actions\n",
        "        actor_target (nn.Module): actor model to predict next actions\n",
        "        actor_optimizer (Optimizer): optimizer for training actor\n",
        "        critic (nn.Module): critic model to predict state values\n",
        "        critic_target (nn.Module): target critic model to predict state values\n",
        "        critic_optimizer (Optimizer): optimizer for training critic\n",
        "        memory (ReplayBuffer): replay memory to store transitions\n",
        "        batch_size (int): batch size for sampling\n",
        "        gamma (float): discount factor\n",
        "        tau (float): parameter for soft target update\n",
        "        initial_random_steps (int): initial random action steps\n",
        "        noise (OUNoise): noise generator for exploration\n",
        "        device (torch.device): cpu / gpu\n",
        "        transition (list): temporory storage for the recent transition\n",
        "        total_step (int): total step numbers\n",
        "        is_test (bool): flag to show the current mode (train / test)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        memory_size: int,\n",
        "        batch_size: int,\n",
        "        ou_noise_theta: float,\n",
        "        ou_noise_sigma: float,\n",
        "        gamma: float = 0.99,\n",
        "        tau: float = 5e-3,\n",
        "        initial_random_steps: int = 1e4,\n",
        "    ):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        obs_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.shape[0]\n",
        "\n",
        "        self.env = env\n",
        "        self.memory = ReplayBuffer(obs_dim, memory_size, batch_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.initial_random_steps = initial_random_steps\n",
        "                \n",
        "        # noise\n",
        "        self.noise = OUNoise(\n",
        "            action_dim,\n",
        "            theta=ou_noise_theta,\n",
        "            sigma=ou_noise_sigma,\n",
        "        )\n",
        "\n",
        "        # device: cpu / gpu\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "        print(self.device)\n",
        "\n",
        "        # networks\n",
        "        self.actor = Actor(obs_dim, action_dim).to(self.device)\n",
        "        self.actor_target = Actor(obs_dim, action_dim).to(self.device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        \n",
        "        self.critic = Critic(obs_dim + action_dim).to(self.device)\n",
        "        self.critic_target = Critic(obs_dim + action_dim).to(self.device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "        # optimizer\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "        \n",
        "        # transition to store in memory\n",
        "        self.transition = list()\n",
        "        \n",
        "        # total steps count\n",
        "        self.total_step = 0\n",
        "\n",
        "        # mode: train / test\n",
        "        self.is_test = False\n",
        "    \n",
        "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Select an action from the input state.\"\"\"\n",
        "        # if initial random action should be conducted\n",
        "        if self.total_step < self.initial_random_steps and not self.is_test:\n",
        "            selected_action = self.env.action_space.sample()\n",
        "        else:\n",
        "            selected_action = self.actor(\n",
        "                torch.FloatTensor(state).to(self.device)\n",
        "            ).detach().cpu().numpy()\n",
        "        \n",
        "        # add noise for exploration during training\n",
        "        if not self.is_test:\n",
        "            noise = self.noise.sample()\n",
        "            selected_action = np.clip(selected_action + noise, -1.0, 1.0)\n",
        "        \n",
        "        self.transition = [state, selected_action]\n",
        "        \n",
        "        return selected_action\n",
        "    \n",
        "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
        "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "        \n",
        "        if not self.is_test:\n",
        "            self.transition += [reward, next_state, done]\n",
        "            self.memory.store(*self.transition)\n",
        "    \n",
        "        return next_state, reward, done\n",
        "    \n",
        "    def update_model(self) -> torch.Tensor:\n",
        "        \"\"\"Update the model by gradient descent.\"\"\"\n",
        "        device = self.device  # for shortening the following lines\n",
        "        \n",
        "        samples = self.memory.sample_batch()\n",
        "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "        action = torch.FloatTensor(samples[\"acts\"].reshape(-1, 1)).to(device)\n",
        "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "        \n",
        "        masks = 1 - done\n",
        "        next_action = self.actor_target(next_state)\n",
        "        next_value = self.critic_target(next_state, next_action)\n",
        "        curr_return = reward + self.gamma * next_value * masks\n",
        "        \n",
        "        # train critic\n",
        "        values = self.critic(state, action)\n",
        "        critic_loss = F.mse_loss(values, curr_return)\n",
        "        \n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "                \n",
        "        # train actor\n",
        "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
        "        \n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # target update\n",
        "        self._target_soft_update()\n",
        "        \n",
        "        return actor_loss.data, critic_loss.data\n",
        "    \n",
        "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
        "        \"\"\"Train the agent.\"\"\"\n",
        "        self.is_test = False\n",
        "        \n",
        "        state = self.env.reset()\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "        scores = []\n",
        "        score = 0\n",
        "        \n",
        "        for self.total_step in range(1, num_frames + 1):\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            # if episode ends\n",
        "            if done:         \n",
        "                state = env.reset()\n",
        "                scores.append(score)\n",
        "                score = 0\n",
        "\n",
        "            # if training is ready\n",
        "            if (\n",
        "                len(self.memory) >= self.batch_size \n",
        "                and self.total_step > self.initial_random_steps\n",
        "            ):\n",
        "                actor_loss, critic_loss = self.update_model()\n",
        "                actor_losses.append(actor_loss)\n",
        "                critic_losses.append(critic_loss)\n",
        "            \n",
        "            # plotting\n",
        "            if self.total_step % plotting_interval == 0:\n",
        "                self._plot(\n",
        "                    self.total_step, \n",
        "                    scores, \n",
        "                    actor_losses, \n",
        "                    critic_losses,\n",
        "                )\n",
        "                \n",
        "        self.env.close()\n",
        "        \n",
        "    def test(self):\n",
        "        \"\"\"Test the agent.\"\"\"\n",
        "        self.is_test = True\n",
        "        \n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "        \n",
        "        frames = []\n",
        "        while not done:\n",
        "            frames.append(self.env.render(mode=\"rgb_array\"))\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "        \n",
        "        print(\"score: \", score)\n",
        "        self.env.close()\n",
        "        \n",
        "        return frames\n",
        "    \n",
        "    def _target_soft_update(self):\n",
        "        \"\"\"Soft-update: target = tau*local + (1-tau)*target.\"\"\"\n",
        "        tau = self.tau\n",
        "        \n",
        "        for t_param, l_param in zip(\n",
        "            self.actor_target.parameters(), self.actor.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
        "            \n",
        "        for t_param, l_param in zip(\n",
        "            self.critic_target.parameters(), self.critic.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
        "    \n",
        "    def _plot(\n",
        "        self, \n",
        "        frame_idx: int, \n",
        "        scores: List[float], \n",
        "        actor_losses: List[float], \n",
        "        critic_losses: List[float], \n",
        "    ):\n",
        "        \"\"\"Plot the training progresses.\"\"\"\n",
        "        def subplot(loc: int, title: str, values: List[float]):\n",
        "            plt.subplot(loc)\n",
        "            plt.title(title)\n",
        "            plt.plot(values)\n",
        "\n",
        "        subplot_params = [\n",
        "            (131, f\"frame {frame_idx}. score: {np.mean(scores[-10:])}\", scores),\n",
        "            (132, \"actor_loss\", actor_losses),\n",
        "            (133, \"critic_loss\", critic_losses),\n",
        "        ]\n",
        "        \n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(30, 5))\n",
        "        for loc, title, values in subplot_params:\n",
        "            subplot(loc, title, values)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "GW1sIaf9Mkv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm Execution"
      ],
      "metadata": {
        "id": "mWavl7GIMqH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_frames = 50000\n",
        "memory_size = 100000\n",
        "batch_size = 128\n",
        "ou_noise_theta = 1.0\n",
        "ou_noise_sigma = 0.1\n",
        "initial_random_steps = 10000\n",
        "\n",
        "agent = DDPGAgent(\n",
        "    env, \n",
        "    memory_size, \n",
        "    batch_size,\n",
        "    ou_noise_theta,\n",
        "    ou_noise_sigma,\n",
        "    initial_random_steps=initial_random_steps\n",
        ")"
      ],
      "metadata": {
        "id": "3dXH12mlMt0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.train(num_frames)"
      ],
      "metadata": {
        "id": "Ya3or16UM0SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "7p0roQPkH46C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# N_User = 20\n",
        "# G_CU_list = np.ones((1,N_User)) \n",
        "# print(G_CU_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reQKX5oIHpwK",
        "outputId": "75618d0f-dde8-498b-e88d-204309d57f3b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([ -57.53854223, -44.63363074,0.        ])\n",
        "B = np.array([-407.76417867,-564.62837335,0.        ])\n",
        "\n",
        "print(\"Expand dims\")\n",
        "A = np.expand_dims(A, axis=0)\n",
        "B = np.expand_dims(B, axis=0)\n",
        "print(A-B)\n",
        "np.sqrt(np.sum((A - B)**2,axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azo5xqDdUaw7",
        "outputId": "7f8525e1-a1e6-4ef4-d573-4da1368794ae"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expand dims\n",
            "[[293.11646614 962.55913185 366.68156459]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1070.93077033])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}